.. Copyright (C) 2001-2015 NLTK Project
.. For license information, see LICENSE.TXT

.. -*- coding: utf-8 -*-

    >>> from pprint import pprint # for dictionary comparison

===============
Counting Ngrams
===============

Building a Vocabulary
---------------------

    >>> from nltk.corpus import gutenberg
    >>> sents = gutenberg.sents("burgess-busterbrown.txt")
    >>> test_sents = sents[3:5]
    >>> test_words = [w for s in test_sents for w in s]
    >>> test_words[:5]
    ['Buster', 'Bear', 'yawned', 'as', 'he']

    >>> from nltk.model import build_vocabulary

The first argument to the `build_vocabulary` function is a cutoff value.

    >>> vocab = build_vocabulary(2, test_words)

This function returns an instance of NgramModelVocabulary, its characteristics
are described below.
Firstly, tokens with counts greater than or equal to the cuttoff value will
be considered part of the vocabulary.

    >>> vocab['the']
    3
    >>> 'the' in vocab
    True
    >>> vocab['he']
    2
    >>> 'he' in vocab
    True

Tokens with frequency counts less than the cutoff value will be considered not
part of the vocabulary even though their entries in the count dictionary are
preserved.

    >>> vocab['Buster']
    1
    >>> 'Buster' in vocab
    False
    >>> vocab['aliens']
    0
    >>> 'aliens' in vocab
    False

Keeping the count entries for seen words allows us to change the cutoff value
without having to recalculate the counts.

    >>> vocab.cutoff = 1
    >>> "Buster" in vocab
    True
    >>> "aliens" in vocab
    False

The cutoff value influences not only membership checking but also the result of
getting the size of the vocabulary using the built-in `len`.
Note that while the number of keys in the vocab dictionary stays the same, passing
it to `len` yields different numbers depending on the cutoff

    >>> len(vocab.keys())
    37
    >>> len(vocab)
    38
    >>> vocab.cutoff = 2
    >>> len(vocab)
    8

Note also that we add 1 to the size of the vocabulary, so even when cutoff=1
`len(vocab) > len(vocab.keys())`. This is done because in many language modeling
algorithms the size of the vocabulary is used for normalizating scores and because
it needs to account for the unknown token label.
This is definitely not an uncotroversial design choice, one that may be reverted
based on discussion and usage.

Vocabulary from multiple sources
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The `build_vocabulary` function can handle multiple text arguments, i.e.

    >>> test_words2 = sents[5]
    >>> test_words3 = sents[6]
    >>> vocab2 = build_vocabulary(2, test_words, test_words2, test_words3)
    >>> len(vocab2)
    14


Counting Ngrams
---------------

    >>> from nltk.model import count_ngrams

The first argument to `count_ngrams` is the highest ngram order to consider, the
second is an NgramModelVocabulary instance, followed by one (or more) texts.
A text is a sequence of sentences, eg. a list of lists of strings.

    >>> bigram_counts = count_ngrams(2, vocab, sents[3:7])

This returns an instance of an NgramCounter class which provides an interface
to the ngram counts.

For all non-unigram ngrams (order > 1) the counts are stored in the `ngrams`
attribute indexed by ngram order.

    >>> bigram_counts.ngrams[2]
    <ConditionalFreqDist with 9 conditions>

The keys of this ConditionalFreqDist are the contexts preceding a word.

    >>> sorted(bigram_counts.ngrams[2].conditions()) # doctest: +NORMALIZE_WHITESPACE
    [('.',), ('<UNK>',), ('<s>',), ('and',),
    ('he',), ('his',), ('the',), ('to',), ('yawned',)]

Each context has a FreqDist of tokens found following it in the text.

    >>> bigram_counts.ngrams[2][('.',)]
    FreqDist({'</s>': 4})

Accessing unigram counts works a little differently because they don't have any
preceding contexts. They are namely stored as a simple FreqDist in the `unigrams`
attribute.

    >>> print(bigram_counts.unigrams)
    <FreqDist with 9 samples and 117 outcomes>
    >>> pprint(bigram_counts.unigrams) # doctest: +NORMALIZE_WHITESPACE
    {'.': 4,
    '</s>': 4,
    '<UNK>': 80,
    'and': 5,
    'he': 6,
    'his': 5,
    'the': 6,
    'to': 4,
    'yawned': 3}

Tweaking the unknown label
~~~~~~~~~~~~~~~~~~~~~~~~~~

While counting ngrams all tokens that do not occur frequently enough (read: are
not "in" the NgramModelVocabulary instance) are replaced by a special "unknown word"
label. By convention this tends to be something like "<UNK>".
This is the default for the NgramCounter and it gets returned by
the `check_against_vocab` method for words that don't pass the cutoff.

    >>> bigram_counts.check_against_vocab('the')
    'the'
    >>> bigram_counts.check_against_vocab('aliens')
    '<UNK>'

This can be changed by passing a different value for the `unk_label` argument.

    >>> diff_unk = count_ngrams(2, vocab, sents[3:7], unk_label="UNKNOWN")
    >>> diff_unk.check_against_vocab('aliens')
    'UNKNOWN'


Changing the cutoff
~~~~~~~~~~~~~~~~~~~

Sometimes we want to compare the effect of different cutoff values on the ngram
counts. We can do this specifying an `unk_cutoff` argument that overrides
the vocabulary's cutoff value.

    >>> cutoff_1 = count_ngrams(2, vocab, unk_cutoff=1)
    >>> cutoff_1.vocabulary.cutoff
    1

Note that this does not affect the original vocabulary cutoff value.

    >>> vocab.cutoff
    2

Changing how ngrams are generated
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NgramCounter uses nltk's `util.ngrams` function to break a text up into ngrams.
The behavior of `util.ngrams` is controlled by passing it keyword arguments stored
in an attribute of NgramCounter.

    >>> pprint(bigram_counts.ngrams_kwargs)  # doctest: +NORMALIZE_WHITESPACE
    {'left_pad_symbol': '<s>',
    'pad_left': True,
    'pad_right': True,
    'right_pad_symbol': '</s>'}

The default values of these arguments have been chosen based on what I think are
the most common practices for ngram splitting. You can easily override the settings
by passing corresponding key=value pairs to `count_ngrams`

    >>> no_padding = count_ngrams(2, vocab, sents[3:6], pad_left=False, pad_right=False)

You can always test your ngram generation by calling the `to_ngrams` method
with an example sentence.

    >>> list(no_padding.to_ngrams(sents[4]))  # doctest: +NORMALIZE_WHITESPACE
    [('Once', 'more'), ('more', 'he'), ('he', 'yawned'), ('yawned', ','),
    (',', 'and'), ('and', 'slowly'), ('slowly', 'got'), ('got', 'to'),
    ('to', 'his'), ('his', 'feet'), ('feet', 'and'), ('and', 'shook'),
    ('shook', 'himself'), ('himself', '.')]

Compare this to the output of the default counter.

    >>> list(bigram_counts.to_ngrams(sents[4]))  # doctest: +NORMALIZE_WHITESPACE
    [('<s>', 'Once'), ('Once', 'more'), ('more', 'he'), ('he', 'yawned'), ('yawned', ','),
    (',', 'and'), ('and', 'slowly'), ('slowly', 'got'), ('got', 'to'),
    ('to', 'his'), ('his', 'feet'), ('feet', 'and'), ('and', 'shook'),
    ('shook', 'himself'), ('himself', '.'), ('.', '</s>')]


Multiple (or no) sources
~~~~~~~~~~~~~~~~~~~~~~~~

Just like `build_vocab`, `count_ngrams` can handle multiple source text arguments.

    >>> multiple_texts_counter = count_ngrams(2, vocab, sents[3:6], sents[6:9])

Moreover, it is also possible to specify no texts whatsoever to simply create
an empty NgramCounter object and train it later.

    >>> no_initial_texts = count_ngrams(2, vocab)

In this case though, it's probably more reader-friendly to use the NgramCounter
class directly:

    >>> from nltk.model.counter import NgramCounter
    >>> no_initial_texts = NgramCounter(2, vocab)

If you choose to do this, you can call the `train_counts` method with some text
to populate your counter instance with some numbers.

    >>> no_initial_texts.train_counts(sents[3:7])
    >>> pprint(no_initial_texts.unigrams) # doctest: +NORMALIZE_WHITESPACE
    {'.': 4,
    '</s>': 4,
    '<UNK>': 80,
    'and': 5,
    'he': 6,
    'his': 5,
    'the': 6,
    'to': 4,
    'yawned': 3}

=====================
From Counts to Scores
=====================

Once we counted up the ngrams it is trivial to turn them into a proper model
with scores (probabilities): just pass your NgramCounter object to a BaseNgramModel
class or any of its children.

    >>> from nltk.model import BaseNgramModel
    >>> bigram_model = BaseNgramModel(bigram_counts)
    >>> bigram_model.ngram_counts == bigram_counts
    True

As its name suggests, the BaseNgramModel class is intended first and foremost
as something folks can base classes for different models. As such BaseNgramModel
doesn't have an interesting `score` method implementation, it always
returns 0.5.

    >>> bigram_model.score("test", ('abc',))
    0.5
    >>> bigram_model.score("abc", ("test",))
    0.5

Children classes are expected to override this method and take advantage of methods
(shown below) whose implementations do not depend on that of `score`.

    >>> bigram_model.logscore("abc", ("test",))
    -1.0
    >>> bigram_model.entropy(sents[4])
    -1.0
    >>> bigram_model.perplexity(sents[4])
    0.5
